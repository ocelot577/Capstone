{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,recall_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('fin_users_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FjolnirFimbulvetr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kazehaya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClawedGiroux</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MrKixs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>icancubutucantcme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0  target\n",
       "0  FjolnirFimbulvetr       0\n",
       "1           Kazehaya       0\n",
       "2       ClawedGiroux       0\n",
       "3             MrKixs       0\n",
       "4  icancubutucantcme       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('fin_users_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jcwinny</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lazigrdnr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boomslangalang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bczeon27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>madosooki</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  target\n",
       "0         jcwinny       0\n",
       "1       lazigrdnr       0\n",
       "2  Boomslangalang       0\n",
       "3        bczeon27       1\n",
       "4       madosooki       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FjolnirFimbulvetr</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kazehaya</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClawedGiroux</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MrKixs</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>icancubutucantcme</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   target\n",
       "Unnamed: 0               \n",
       "FjolnirFimbulvetr       0\n",
       "Kazehaya                0\n",
       "ClawedGiroux            0\n",
       "MrKixs                  0\n",
       "icancubutucantcme       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = pd.read_csv('fin_cleaned_nlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PDXorax</td>\n",
       "      <td>The Lump of Labor doesn't apply in this case,...</td>\n",
       "      <td>the lump of labor doesn apply in this case we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PrincePizza1</td>\n",
       "      <td>I think things \"getting better\" to people lik...</td>\n",
       "      <td>think things getting better to people like thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marmar79</td>\n",
       "      <td>Thank you. This made my day He so edgy.\\n\\nEd...</td>\n",
       "      <td>thank you this made my day he so edgy edit don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>IRISHE3</td>\n",
       "      <td>I just wish politicians would put the money s...</td>\n",
       "      <td>just wish politicians would put the money shit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>RonZiggy</td>\n",
       "      <td>Climate change as well. Wow...just wow What i...</td>\n",
       "      <td>climate change as well wow just wow what is th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          User  \\\n",
       "0           0       PDXorax   \n",
       "1           1  PrincePizza1   \n",
       "2           2      Marmar79   \n",
       "3           3       IRISHE3   \n",
       "4           4      RonZiggy   \n",
       "\n",
       "                                                Text  \\\n",
       "0   The Lump of Labor doesn't apply in this case,...   \n",
       "1   I think things \"getting better\" to people lik...   \n",
       "2   Thank you. This made my day He so edgy.\\n\\nEd...   \n",
       "3   I just wish politicians would put the money s...   \n",
       "4   Climate change as well. Wow...just wow What i...   \n",
       "\n",
       "                                               Clean  \n",
       "0  the lump of labor doesn apply in this case we ...  \n",
       "1  think things getting better to people like thi...  \n",
       "2  thank you this made my day he so edgy edit don...  \n",
       "3  just wish politicians would put the money shit...  \n",
       "4  climate change as well wow just wow what is th...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp.set_index('User',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "Text          0\n",
       "Clean         3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords and lemmatize the NLP dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add candidates to the stop words list since these will greatly skew results and not help us identify the topics they talk about\n",
    "stops.update(['yang','andrew','pete','buttigieg','mayor','sanders','bernie','bern','biden','kamala','harris','warren','elizabeth','tulsi','gabbard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don can be a nickname for donald and has relevance for this project\n",
    "stops.remove('don')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual function that tokenizes text, removes stop words, lemmatizes the tokens, and returns the tokens joined with a space\n",
    "def stop_and_lemma(text):\n",
    "    processed_tokens=[]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in stops:\n",
    "            processed_tokens.append(lemmatizer.lemmatize(token))\n",
    "    return(\" \".join(processed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp['Use'] = df_nlp['Clean'].apply(stop_and_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nlp.drop(labels=['Unnamed: 0','Text','Clean'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using inner joins here to remove out users who did not have a post in r/politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train,df_nlp],join='inner',axis=1)\n",
    "df_train['predicted_target']=df_train['target']\n",
    "df_train.drop(labels=['target'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test,df_nlp],join='inner',axis=1)\n",
    "df_test['predicted_target']=df_test['target']\n",
    "df_test.drop(labels=['target'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying TF-IDF vectorizer, Logisitic Regression Classifier and hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores For:  (1, 1) 100\n",
      "0.797289972899729\n",
      "0.7850162866449512\n",
      "Scores For:  (1, 1) 500\n",
      "0.8509485094850948\n",
      "0.8127035830618893\n",
      "Scores For:  (1, 1) 1000\n",
      "0.8612466124661247\n",
      "0.8175895765472313\n",
      "Scores For:  (1, 2) 100\n",
      "0.797289972899729\n",
      "0.7850162866449512\n",
      "Scores For:  (1, 2) 500\n",
      "0.8504065040650407\n",
      "0.8127035830618893\n",
      "Scores For:  (1, 2) 1000\n",
      "0.8596205962059621\n",
      "0.8208469055374593\n",
      "Scores For:  (1, 3) 100\n",
      "0.797289972899729\n",
      "0.7850162866449512\n",
      "Scores For:  (1, 3) 500\n",
      "0.8504065040650407\n",
      "0.8127035830618893\n",
      "Scores For:  (1, 3) 1000\n",
      "0.8596205962059621\n",
      "0.8208469055374593\n"
     ]
    }
   ],
   "source": [
    "#Due to the multiple operations that needed to be run on these dataframes between the vecotorizing and\n",
    "## the logisitic regression, I opted to use a for loop to tune my hyperparameters as opposed to a grid search\n",
    "for n_grams in [(1,1),(1,2),(1,3)]:\n",
    "    for features in [100,500,1000]:\n",
    "        #initialize TF-IDF vectorizer\n",
    "        tf = TfidfVectorizer(ngram_range = n_grams,max_features=features)\n",
    "        #Generate sparse matrixes and combine with previous features\n",
    "        sparseXtrain = tf.fit_transform(df_train['Use'])\n",
    "        sparseXtrain_df= pd.DataFrame(sparseXtrain.toarray(),\n",
    "               columns=tf.get_feature_names(),index=df_train.index)\n",
    "        df_train2 = pd.concat([df_train.drop('Use',axis=1),sparseXtrain_df],axis=1,join='inner')\n",
    "\n",
    "        sparseXtest = tf.transform(df_test['Use'])\n",
    "        sparseXtest_df= pd.DataFrame(sparseXtest.toarray(),\n",
    "               columns=tf.get_feature_names(),index=df_test.index)\n",
    "        df_test2 = pd.concat([df_test.drop('Use',axis=1),sparseXtest_df],axis=1,join='inner')\n",
    "\n",
    "        #Create X_Train/y_train\n",
    "        X_train = df_train2.drop('predicted_target',axis=1)\n",
    "        X_test = df_test2.drop('predicted_target',axis=1)\n",
    "        y_train = df_train2['predicted_target']\n",
    "        y_test = df_test2['predicted_target']\n",
    "\n",
    "        #Run logisitic Regression\n",
    "        lr = LogisticRegression(solver='lbfgs')\n",
    "        lr.fit(X_train,y_train)\n",
    "        print('Scores For: ',n_grams,features)\n",
    "        print(lr.score(X_train,y_train))\n",
    "        print(lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer model generally doesn't converge, scores look worse than TF-IDF so not using\n",
    "\n",
    "# for n_grams in [(1,1),(1,2)]:\n",
    "#     for features in [100,500]:\n",
    "#         #initialize Count Vectorizer\n",
    "#         cv = CountVectorizer(ngram_range = n_grams,max_features=features)\n",
    "#         #Generate sparse matrixes and combine with previous features\n",
    "#         sparseXtrain = cv.fit_transform(df_train['Use'])\n",
    "#         sparseXtrain_df= pd.DataFrame(sparseXtrain.toarray(),\n",
    "#                columns=cv.get_feature_names(),index=df_train.index)\n",
    "#         df_train2 = pd.concat([df_train.drop('Use',axis=1),sparseXtrain_df],axis=1,join='inner')\n",
    "\n",
    "#         sparseXtest = cv.transform(df_test['Use'])\n",
    "#         sparseXtest_df= pd.DataFrame(sparseXtest.toarray(),\n",
    "#                columns=cv.get_feature_names(),index=df_test.index)\n",
    "#         df_test2 = pd.concat([df_test.drop('Use',axis=1),sparseXtest_df],axis=1,join='inner')\n",
    "\n",
    "#         #Create X_Train/y_train\n",
    "#         X_train = df_train2.drop('predicted_target',axis=1)\n",
    "#         X_test = df_test2.drop('predicted_target',axis=1)\n",
    "#         y_train = df_train2['predicted_target']\n",
    "#         y_test = df_test2['predicted_target']\n",
    "\n",
    "#         #Run logisitic Regression\n",
    "#         lr = LogisticRegression(solver='lbfgs')\n",
    "#         lr.fit(X_train,y_train)\n",
    "#         print(lr.score(X_test,y_test),\"variables: \",n_grams,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores For: (1,2) & 1000 features\n",
      "0.8596205962059621\n",
      "0.8208469055374593\n"
     ]
    }
   ],
   "source": [
    "#Using the TF-IDF vecotrizer that had the best score and ran in the shortest time\n",
    "tf = TfidfVectorizer(ngram_range = (1,2),max_features=1000)\n",
    "        \n",
    "#Generate sparse matrixes and combine with previous features\n",
    "sparseXtrain = tf.fit_transform(df_train['Use'])\n",
    "sparseXtrain_df= pd.DataFrame(sparseXtrain.toarray(),\n",
    "       columns=tf.get_feature_names(),index=df_train.index)\n",
    "df_train2 = pd.concat([df_train.drop('Use',axis=1),sparseXtrain_df],axis=1,join='inner')\n",
    "\n",
    "sparseXtest = tf.transform(df_test['Use'])\n",
    "sparseXtest_df= pd.DataFrame(sparseXtest.toarray(),\n",
    "       columns=tf.get_feature_names(),index=df_test.index)\n",
    "df_test2 = pd.concat([df_test.drop('Use',axis=1),sparseXtest_df],axis=1,join='inner')\n",
    "\n",
    "#Create X_Train/y_train\n",
    "X_train = df_train2.drop('predicted_target',axis=1)\n",
    "X_test = df_test2.drop('predicted_target',axis=1)\n",
    "y_train = df_train2['predicted_target']\n",
    "y_test = df_test2['predicted_target']\n",
    "\n",
    "#Run logisitic Regression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train,y_train)\n",
    "print('Scores For: (1,2) & 1000 features')\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[456,  10],\n",
       "       [100,  48]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    466\n",
       "1    148\n",
       "Name: predicted_target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.758958\n",
       "1    0.241042\n",
       "Name: predicted_target, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32432432432432434"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unbalanced classes are causing us to underpreict on the positive class, let's use an oversampling technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Oversampling of Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bal,y_train_bal = smt.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsmt = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrsmt.fit(X_train_bal,y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8600572655690766"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrsmt.score(X_train_bal,y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.737785016286645"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrsmt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[347, 119],\n",
       "       [ 42, 106]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,lrsmt.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7162162162162162"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,lrsmt.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ubi</th>\n",
       "      <td>6.715239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>welfare</th>\n",
       "      <td>2.733742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debate</th>\n",
       "      <td>2.643199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solution</th>\n",
       "      <td>2.155616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freedom</th>\n",
       "      <td>2.020890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>1.861347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>1.669680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>1.608740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>1.480669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polling</th>\n",
       "      <td>1.372441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current</th>\n",
       "      <td>1.314059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economy</th>\n",
       "      <td>1.257654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>least</th>\n",
       "      <td>1.256418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>1.220974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <td>1.166791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currently</th>\n",
       "      <td>1.134536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>1.132763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>1.117282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>1.116906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>1.105945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>1.095280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basic</th>\n",
       "      <td>1.064067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1.061620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smart</th>\n",
       "      <td>1.007105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>0.987402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interview</th>\n",
       "      <td>0.963251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kavanaugh</th>\n",
       "      <td>0.959356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.953807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.922079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wing</th>\n",
       "      <td>-1.189738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>-1.198998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>-1.200071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gop</th>\n",
       "      <td>-1.202804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>-1.205956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>-1.222591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-1.223976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>-1.249094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>since</th>\n",
       "      <td>-1.280983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem</th>\n",
       "      <td>-1.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>republican</th>\n",
       "      <td>-1.312360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headline</th>\n",
       "      <td>-1.315265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faith</th>\n",
       "      <td>-1.320276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>history</th>\n",
       "      <td>-1.360050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thanks</th>\n",
       "      <td>-1.384268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticket</th>\n",
       "      <td>-1.396087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>party</th>\n",
       "      <td>-1.416556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>-1.438322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>around</th>\n",
       "      <td>-1.461663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democrat</th>\n",
       "      <td>-1.490477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary</th>\n",
       "      <td>-1.500655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>progressive</th>\n",
       "      <td>-1.521834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>often</th>\n",
       "      <td>-1.576711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank</th>\n",
       "      <td>-1.607691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort</th>\n",
       "      <td>-1.649351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corporate</th>\n",
       "      <td>-1.683480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hillary</th>\n",
       "      <td>-1.741882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>-1.749631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democratic</th>\n",
       "      <td>-1.823155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuck</th>\n",
       "      <td>-1.946718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "ubi          6.715239\n",
       "welfare      2.733742\n",
       "debate       2.643199\n",
       "solution     2.155616\n",
       "freedom      2.020890\n",
       "job          1.861347\n",
       "program      1.669680\n",
       "business     1.608740\n",
       "could        1.480669\n",
       "polling      1.372441\n",
       "current      1.314059\n",
       "economy      1.257654\n",
       "least        1.256418\n",
       "help         1.220974\n",
       "policy       1.166791\n",
       "currently    1.134536\n",
       "population   1.132763\n",
       "fire         1.117282\n",
       "many         1.116906\n",
       "need         1.105945\n",
       "problem      1.095280\n",
       "basic        1.064067\n",
       "month        1.062500\n",
       "data         1.061620\n",
       "smart        1.007105\n",
       "address      0.987402\n",
       "interview    0.963251\n",
       "kavanaugh    0.959356\n",
       "like         0.953807\n",
       "finally      0.922079\n",
       "...               ...\n",
       "wing        -1.189738\n",
       "time        -1.198998\n",
       "year        -1.200071\n",
       "gop         -1.202804\n",
       "israel      -1.205956\n",
       "state       -1.222591\n",
       "best        -1.223976\n",
       "voter       -1.249094\n",
       "since       -1.280983\n",
       "dem         -1.281900\n",
       "republican  -1.312360\n",
       "headline    -1.315265\n",
       "faith       -1.320276\n",
       "history     -1.360050\n",
       "thanks      -1.384268\n",
       "ticket      -1.396087\n",
       "party       -1.416556\n",
       "south       -1.438322\n",
       "around      -1.461663\n",
       "democrat    -1.490477\n",
       "primary     -1.500655\n",
       "progressive -1.521834\n",
       "often       -1.576711\n",
       "thank       -1.607691\n",
       "sort        -1.649351\n",
       "corporate   -1.683480\n",
       "hillary     -1.741882\n",
       "house       -1.749631\n",
       "democratic  -1.823155\n",
       "fuck        -1.946718\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resulting dataframe showing words with the most predictive power\n",
    "pd.DataFrame(lrsmt.coef_.T,X_train.columns).sort_values(by=0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results aren't that surprising. The strongest word associated with Yang supporters is \"ubi\" which stands for universal basic income and is the center topic in Andrew Yang's platform. Other highly associated words are \"welfare\" (perhaps indicating Yang supporters discussing how ubi would interact with current welfare platforms or how ubi isn't the same as welfare), \"debate\" (a big topic of Yang supporters the last couple months are how he did in the July debates and if he'll qualify for the Sept/Oct debates), and \"freedom\" (Andrew's ubi proposal is known as the \"freedom dividend\"). Words that are not associated with Yang supporters are a bit more interesting and harder to explain. The word that's least associated with Yang supporters is \"f&ast;&ast;&ast;\". One guess for this is \"f&ast;&ast;&ast;\" is often used in conjunction with Trump. Many supporters of Yang are known to be more moderate or even previous Trump supporters so one would expect Yang supporters to probably be critical of the president and his politics but less outright hostile and willing to drop blanket anti-Trump phrases such as \"f&ast;&ast;&ast; Trump\" or \"f&ast;&ast;&ast; republicans\". Another interesting word here is \"corporate\", perhaps indicating that Yang supporters are less likely to complain about corporations and corporate influence in politics than non-Yang supporters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
